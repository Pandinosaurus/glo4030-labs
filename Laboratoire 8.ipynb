{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78bdaafd",
   "metadata": {},
   "source": [
    "### Utilisation de Google Colab\n",
    "\n",
    "Si vous utilisez Google Colab, suivez les instructions ci-dessous.\n",
    "\n",
    "Tout d'abord, sélectionnez l'option GPU de Colab avec *Edit > Notebook settings* et sélectionner GPU comme Hardware accelerator. Installer ensuite deeplib avec la commande suivante:"
   ]
  },
  {
   "cell_type": "code",
   "id": "05c04f14",
   "metadata": {},
   "source": [
    "!pip install git+https://github.com/ulaval-damas/glo4030-labs.git"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1c729b9f",
   "metadata": {},
   "source": [
    "# Laboratoire 8: Transformers\n",
    "\n",
    "Dans ce laboratoire, nous allons implémenter l'architecture Transformer de \"Attention Is All You Need\" (Vaswani, 2017)."
   ]
  },
  {
   "cell_type": "code",
   "id": "7e48c943",
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pathlib\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import einops as ein\n",
    "\n",
    "import deeplib.training as dtrain\n",
    "import deeplib.datasets as ddatasets"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aae58cdb",
   "metadata": {},
   "source": [
    "## Partie 1: Scaled Dot-Product Attention\n",
    "\n",
    "Au cœur des transformers se trouve le principe d'attention, qui permet de se concentrer sur certaines parties de l'information plutôt que sur d'autres.\n",
    "Dans le cas des transformers, on utilise le \"Scaled Dot-Product Attention\" décrit dans \"Attention Is All You Need\".\n",
    "Ce principe fonctionne comme une requête sur une hashmap: on fait une requête $q$ (query) selon des clés $k_i$ (keys), et on reçoit la valeur $v_i$ (values) associée à $q$.\n",
    "Plus formellement, on peut représenter la hashmap comme un ensemble de pair (clé, valeur) $D = \\{(k_1, v_1), \\ldots (k_m, v_m)\\}$, où $k_i$ sont les clés et $v_i$ leur valeur associée.\n",
    "Ainsi, lorsque l'on fait la requête $q$ sur la base de données $D$, on obtient la valeur associée à $q$.\n",
    "\n",
    "L'attention est similaire à ce principle, sauf que $q$, $k_i$ et $v_i$ sont tous des vecteurs, et la réponse est une somme pondérée de $v_i$.\n",
    "Ainsi, l'attention calcule la fonction suivante:\n",
    "$$Transformer(q, D) = \\sum_{i=1}^{m}{\\alpha(q, k_i)v_i}$$\n",
    "\n",
    "L'attention portée à une valeur est donnée par $\\alpha(q, k_i)$.\n",
    "Il s'agit en quelque sorte d'une mesure de la similarité entre la requête $q$ et les clés $k_i$.\n",
    "Comme le nom \"Scaled Dot-Product Attention\" laisse deviner, cette similarité, $\\alpha(\\cdot)$, est un produit scalaire entre les vecteurs $q$ et $k_i$:\n",
    "\n",
    "$$\\alpha(q, k_i) = \\frac{qk_i^T}{\\sqrt{d}} = \\frac{q \\cdot k_i }{\\sqrt{d}}$$\n",
    "\n",
    "$d$ est le nombre de dimension des vecteurs. On justifiera le facteur d'échelle de $\\frac{1}{\\sqrt{d}}$ plus tard dans le laboratoire.\n",
    "\n",
    "\n",
    "On normalise souvent cette valeur à l'aide d'une softmax afin de garder chaque pondération positive et sommant à 1.\n",
    "\n",
    "$$\\alpha(q, k_i) = \\frac{\\exp(\\alpha(q, k_i))}{\\sum_j\\exp(\\alpha(q, k_j))}$$\n",
    "\n",
    "Ainsi, le calcul d'attention dans un Transformer est:\n",
    "\n",
    "$$\\alpha(q, k_i) = \\frac{\\exp\\left(q \\cdot k_i / \\sqrt{d}\\right)}{\\sum_j\\exp\\left(q \\cdot k_j / \\sqrt{d}\\right)}$$\n",
    "\n",
    "Ensuite, il est possible de prendre une somme pondérée des valeurs $v_i$ selon $\\alpha(q, k_i)$:\n",
    "\n",
    "$$Transformer(q, D) = \\sum_{i=1}^{m}{\\alpha(q, k_i)v_i}$$\n",
    "\n",
    "Dans la cellule suivante, remplisser la fonction suivante pour appliquer l'attention sur les tenseurs $q$, $k$ et $v$.\n",
    "Les dimensions des tenseurs sont spécifiés en commentaire.\n",
    "Utilisez `F.softmax` pour normaliser l'attention.\n",
    "\n",
    "Assurez-vous que votre calcul d'attention donne bien la valeur attendue."
   ]
  },
  {
   "cell_type": "code",
   "id": "ab97de76",
   "metadata": {},
   "source": [
    "def simple_attention(q, k, v):\n",
    "    \"\"\"\n",
    "    Attention simple\n",
    "    q est un tenseur de dimension (1, d)\n",
    "    k est un tenseur de dimension (n, d)\n",
    "    v est un tenseur de dimension (n, d)\n",
    "    d est la dimension des vecteurs, n le nombre de clés et de valeurs\n",
    "    La sortie devrait être de dimension (1, d)\n",
    "    \"\"\"\n",
    "    # TODO calculer le scaled-dot product attention\n",
    "    return\n",
    "    \n",
    "q = F.normalize(torch.tensor([[3.0, 3.0]]), dim=1)\n",
    "k = F.normalize(torch.tensor([[2.0, 1.5], [-4, 3], [0, 3], [-1, -4]]), dim=1)\n",
    "v = F.normalize(torch.tensor([[-1.0, 2], [3, 4], [-2, -1], [0, 2]]), dim=1)\n",
    "\n",
    "expected = torch.tensor([[-0.3584,  0.4563]])\n",
    "output = simple_attention(q, k, v)\n",
    "\n",
    "print(f'simple_attention(q, k, v) =\\t{output}')\n",
    "print(f'valeur attendue =\\t\\t{expected}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4f279151",
   "metadata": {},
   "source": [
    "### Visualisation de l'attention\n",
    "Le produit scalaire peut aussi s'écrire sous la forme $q\\cdot k_i = \\|q\\|\\|k_i\\|\\cos(\\theta)$, où $\\theta$ est l'angle entre les vecteurs.\n",
    "Si on fait l'hypothèse que les vecteurs en jeux sont unitaires, alors on a que le produit scalaire entre deux donne une mesure d'à quel point les vecteurs sont alignés.\n",
    "On utilise cette interprétation du produit scalaire pour mieux visualiser ce que fait le Scaled Dot-Product Attention.\n",
    "\n",
    "Dans la cellule suivante, remplissez le `TODO` avec le calcul pour calculer `alpha`, le vecteur d'attention."
   ]
  },
  {
   "cell_type": "code",
   "id": "dd2934b4",
   "metadata": {},
   "source": [
    "def show_attention(q, k, v):\n",
    "    # TODO calculer le vecteur d'attention, après la softmax\n",
    "    d = q.size(-1)\n",
    "    alpha = ...\n",
    "\n",
    "    figs, axes = plt.subplots(ncols=2, figsize=(14, 6))\n",
    "    ax = axes[0]\n",
    "    ax.arrow(0, 0, q[0][0], q[0][1], head_width=0.1, head_length=0.1, fc='C4', ec='C4', label='$q$')\n",
    "\n",
    "    colors = ['C0', 'C1', 'C2', 'C3']\n",
    "    for i, k_i in enumerate(k):\n",
    "        ax.arrow(0, 0, k_i[0], k_i[1], head_width=0.1, head_length=0.1, label=f'$k_{i}$, $\\\\alpha(q, k_{i}) = {alpha[0][i]:.3f}$', fc=colors[i], ec=colors[i])\n",
    "\n",
    "    ax.set_xlim(-1.25, 1.25)\n",
    "    ax.set_ylim(-1.25, 1.25)\n",
    "    ax.set_xlabel('X-axis')\n",
    "    ax.set_ylabel('Y-axis')\n",
    "    ax.set_title('$q$ and $k_i$')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "    ax = axes[1]\n",
    "    ax.arrow(0, 0, output[0][0], output[0][1], head_width=0.1, head_length=0.1, fc='C4', ec='C4', label='$\\sum_i v_{i} \\cdot \\\\alpha(q, k_{i})$')\n",
    "\n",
    "    for i, v_i in enumerate(v):\n",
    "        v_i = v_i * alpha[0, i]\n",
    "        ax.arrow(0, 0, v_i[0], v_i[1], head_width=0.1, head_length=0.1, label=f'$v_{i} \\cdot \\\\alpha(q, k_{i})$', fc=colors[i], ec=colors[i])\n",
    "\n",
    "    ax.set_xlim(-1.25, 1.25)\n",
    "    ax.set_ylim(-1.25, 1.25)\n",
    "    ax.set_xlabel('X-axis')\n",
    "    ax.set_ylabel('Y-axis')\n",
    "    ax.set_title(r'Valeurs pondérées par $\\alpha$')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "show_attention(q, k, v)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7da42f1b",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "- Visuellement, quel clé est la plus alignée avec $q$? Que peut-on dire de la valeur de $\\alpha$ correspondante? Qu'est-ce que cela veut-dire à propose de la valeur finale?\n",
    "- Que ce passe-t'il si vous changez tous les $k_i$ afin qu'ils soient opposés à $q$, sauf un? Quel est l'impact sur la valeur retournée par l'attention?\n",
    "- Que ce passe-t'il si vous changez tous les $k_i$ afin qu'ils soient opposés à $q$, sauf **deux** qui ont des valeurs égales? Quel est l'impact sur la valeur retournée par l'attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d5198f",
   "metadata": {},
   "source": [
    "### Plusieurs requêtes\n",
    "\n",
    "Maintenant, on veut implémenter l'attention avec plusieurs requêtes $q_i$.\n",
    "On veut comparer chaque $q_i$ avec chaque $k_i$.\n",
    "Afin d'accélérer le calcul de l'attention, on va représenter cette opération grâce à une multiplication de tenseurs.\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V$$\n",
    "\n",
    "$$Q=\\begin{pmatrix}\n",
    "q_1\\\\\n",
    "q_2\\\\\n",
    "\\vdots\\\\\n",
    "q_n\n",
    "\\end{pmatrix}_{n\\times d}, K=\\begin{pmatrix}\n",
    "k_1\\\\\n",
    "k_2\\\\\n",
    "\\vdots\\\\\n",
    "k_n\n",
    "\\end{pmatrix}_{n\\times d}, V=\\begin{pmatrix}\n",
    "v_1\\\\\n",
    "v_2\\\\\n",
    "\\vdots\\\\\n",
    "v_n\n",
    "\\end{pmatrix}_{n\\times d}$$\n",
    "\n",
    "Chaque $q_i$, $k_i$ et $v_i$ sont des vecteurs de dimensions $d$.\n",
    "\n",
    "La comparaison de chaque $q_i$ et $k_i$ se fait lors du produit suivant:\n",
    "\n",
    "$$QK^T = \\begin{pmatrix}\n",
    "q_1\\\\\n",
    "q_2\\\\\n",
    "\\vdots\\\\\n",
    "q_n\n",
    "\\end{pmatrix}_{n\\times d} \\begin{pmatrix}\n",
    "k_1 & k_2 & \\dots & k_n\n",
    "\\end{pmatrix}_{d\\times n} = \\begin{pmatrix}\n",
    "q_1k_1 & q_1k_2 & \\dots  & q_1k_n \\\\\n",
    "q_2k_1 & q_2k_2 & \\dots  & q_2k_n \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "q_nk_1 & q_nk_2 & \\dots  & q_nk_n \\\\\n",
    "\\end{pmatrix}_{n\\times n}$$\n",
    "\n",
    "On appelle $QK^T$ la matrice d'attention.\n",
    "Chaque élément de cette matrice correspond au produit scalaire entre une requête et une clé.\n",
    "\n",
    "Si on ignore la softmax et le facteur d'échelle, on obtient bel et bien une somme pondérée des valeurs, selon la similarité entre les clés et les requêtes.\n",
    "\n",
    "$$QK^TV = \\begin{pmatrix}\n",
    "q_1k_1 & q_1k_2 & \\dots  & q_1k_n \\\\\n",
    "q_2k_1 & q_2k_2 & \\dots  & q_2k_n \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "q_nk_1 & q_nk_2 & \\dots  & q_nk_n \\\\\n",
    "\\end{pmatrix}_{n\\times n}\\begin{pmatrix}\n",
    "v_1\\\\\n",
    "v_2\\\\\n",
    "\\vdots\\\\\n",
    "v_n\n",
    "\\end{pmatrix}_{n\\times d}=\\begin{pmatrix}\n",
    "q_1k_1v_1 + q_1k_2v_2 + \\dots  + q_1k_nv_n \\\\\n",
    "q_2k_1v_1 + q_2k_2v_2 + \\dots  + q_2k_nv_n \\\\\n",
    "\\vdots \\\\\n",
    "q_nk_1v_1 + q_nk_2v_2 + \\dots  + q_nk_nv_n \\\\\n",
    "\\end{pmatrix}_{n\\times d}$$\n",
    "\n",
    "Notez que l'application de la softmax se fera sur la dernière dimension de la matrice d'attention.\n",
    "\n",
    "Dans la cellule suivante, implémenter l'attention en batch avec plusieurs requêtes.\n",
    "Les dimensions des tenseurs sont spécifiés en commentaire.\n",
    "\n",
    "Assurez-vous que votre calcul d'attention donne bien la valeur attendue."
   ]
  },
  {
   "cell_type": "code",
   "id": "4d11c2d2",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    Attention sur batch\n",
    "    q est un tenseur de dimension (b, n, d)\n",
    "    k est un tenseur de dimension (b, n, d)\n",
    "    v est un tenseur de dimension (b, n, d)\n",
    "    b est la taille de la mini-batch, n le nombre de clés et de valeurs, d est la dimension des vecteurs\n",
    "    La sortie devrait être de dimension (b, n, d)\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    attention = ...\n",
    "    if mask is not None:\n",
    "        # NE PAS FAIRE TOUT DE SUITE, voir plus bas.\n",
    "        # Commencez par calculer la matrice d'attention sans le masque\n",
    "        # TODO appliquer un masque sur `attention`, détails dans la cellule suivante\n",
    "        ...\n",
    "    \n",
    "    # TODO calculer la somme pondérée de v selon attention\n",
    "    return\n",
    "\n",
    "q = F.normalize(torch.tensor([[3.0, 3.0], [-1, 3.2], [0.2, -0.4], [0, -1]]), dim=1).unsqueeze(0)\n",
    "k = F.normalize(torch.tensor([[2.0, 1.5], [-4, 3], [0, 3], [-1, -4]]), dim=1).unsqueeze(0)\n",
    "v = F.normalize(torch.tensor([[-1.0, 2], [3, 4], [2, -3], [0, 2]]), dim=1).unsqueeze(0)\n",
    "\n",
    "expected = torch.tensor([[[0.1089, 0.3322], [0.2860, 0.2639], [0.0600, 0.6791], [0.0986, 0.7087]]])\n",
    "output = scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "print(f'scaled_dot_product_attention(q, k, v) =\\n{output}')\n",
    "print(f'valeur attendue =\\n{expected}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b962ca3a",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "- Quelle est la taille du tenseur lorsque l'on multiple $q$ et $v$? Est-ce problématique pour le temps de calculs et la mémoire utilisée si $n$ (le nombre de queries, keys et valeurs) augmente?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e499e915",
   "metadata": {},
   "source": [
    "#### Masques\n",
    "Implémentez l'opération de masque dans la fonction `scaled_dot_product_attention`.\n",
    "Le masque permet d'empêcher certaines requêtes de porter attention à certaines valeurs.\n",
    "\n",
    "Ceci est possible en remplaçant les éléments de la matrice d'attention par $-\\infty$ où `mask` est `False`"
   ]
  },
  {
   "cell_type": "code",
   "id": "0002375a",
   "metadata": {},
   "source": [
    "q = F.normalize(torch.tensor([[3.0, 3.0], [-1, 3.2], [0.2, -0.4], [0, -1]]), dim=1).unsqueeze(0)\n",
    "k = F.normalize(torch.tensor([[2.0, 1.5], [-4, 3], [0, 3], [-1, -4]]), dim=1).unsqueeze(0)\n",
    "v = F.normalize(torch.tensor([[-1.0, 2], [3, 4], [2, -3], [0, 2]]), dim=1).unsqueeze(0)\n",
    "mask = torch.tensor([[1, 0, 0, 0], [1, 1, 0, 0], [1, 1, 1, 0], [1, 1, 1, 1]], dtype=torch.bool)\n",
    "\n",
    "expected = torch.tensor([[[-0.4472,  0.8944], [ 0.1639,  0.8393], [ 0.1129,  0.3967], [ 0.0986,  0.7087]]])\n",
    "output = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "print(f'scaled_dot_product_attention(q, k, v) =\\n{output}')\n",
    "print(f'valeur attendue =\\n{expected}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c467e9c4",
   "metadata": {},
   "source": [
    "#### Visualisation de l'attention\n",
    "Il est aussi possible de visualiser directement la matrice d'attention.\n",
    "\n",
    "Dans la cellule suivate, calculez l'attention avec un masque."
   ]
  },
  {
   "cell_type": "code",
   "id": "f29419ef",
   "metadata": {},
   "source": [
    "q = F.normalize(torch.tensor([[3.0, 3.0], [-1, 3.2], [0.2, -0.4], [0, -1]]), dim=1).unsqueeze(0)\n",
    "k = F.normalize(torch.tensor([[2.0, 1.5], [-4, 3], [0, 3], [-1, -4]]), dim=1).unsqueeze(0)\n",
    "v = F.normalize(torch.tensor([[-1.0, 2], [3, 4], [2, -3], [0, 2]]), dim=1).unsqueeze(0)\n",
    "mask = torch.tensor([[1, 0, 0, 0], [1, 1, 0, 0], [1, 1, 1, 0], [1, 1, 1, 1]], dtype=torch.bool)\n",
    "# mask = torch.ones((4, 4), dtype=torch.bool)\n",
    "\n",
    "# TODO\n",
    "attention = ...\n",
    "softmax = ...\n",
    "output = ...\n",
    "\n",
    "qs = [f'$q_{i}$' for i in range(q.size(1))]\n",
    "ks = [f'$k_{i}$' for i in range(q.size(1))]\n",
    "\n",
    "plt.imshow((softmax[0]))\n",
    "\n",
    "plt.xticks(range(q.size(1)), ks)\n",
    "plt.yticks(range(q.size(1)), qs)\n",
    "\n",
    "plt.xlabel('Keys')\n",
    "plt.ylabel('Queries')\n",
    "\n",
    "plt.colorbar()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e0a215d7",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "- Que se passe-t'il lors de l'ajout d'un masque?\n",
    "\n",
    "- Pour les questions suivantes, n'utilisez pas de masque.\n",
    "\n",
    "- Que se passe-t'il si on change l'ordre des requêtes $q$? Vous pouvez changer l'ordre manuellement avec `q = q[:, [1, 0, 2, 3]]`.\n",
    "\n",
    "- Que se passe-t'il si on change l'ordre des clés et des valeurs? Vous devez permuter $k$ et $v$ de la même façon.\n",
    "\n",
    "- Est-ce l'invariance à l'ordre des entrées est problématique? Quant est-t'il si l'ordre est important? par exemple en traitement de la langue naturelle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653ebbfa",
   "metadata": {},
   "source": [
    "### Encodage de position\n",
    "Comme on vient de le remarquer, les Transformers sont invariants à la permutation des $q_i$ et des paires $(k_i, v_i)$, on doit donc encoder l'information sur la position directement dans ces vecteurs.\n",
    "Pour ce faire, on utilise une suite de sinus et de cosinus.\n",
    "\n",
    "L'idée est de générer des vecteurs où chaque composante oscille à des fréquences différentes.\n",
    "Comme illustré dans la figure suivante issue de [Visual Guide to Transformer Neural Networks, Hedu AI](https://www.youtube.com/watch?v=dichIcUZfOw), chaque vecteur (une slice verticale dans le graphique) permet de représenter une position dans la séquence.\n",
    "Les composantes avec une fréquence faible (en bleu) permettent d'avoir un estimé de la position sur l'ensemble de la séquence, mais avec peu de précision.\n",
    "Les composantes avec une fréquence élevée (en rouge) permettent d'avoir une estimation plus précise de la position, mais seulement sur une petite partie de la séquence puisque les valeurs se répètent périodiquement."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(8, 8), sharex=True)\n",
    "\n",
    "# Define the x values (word positions) and different frequencies\n",
    "x = np.linspace(0, 10, 1000)\n",
    "y1 = np.sin(10 * x)  # High frequency for i = 0\n",
    "y2 = np.sin(5 * x)   # Medium frequency for i = 2\n",
    "y3 = np.sin(2 * x)   # Low frequency for i = 4\n",
    "\n",
    "# Plot the sine waves on individual axes\n",
    "axs[0].plot(x, y1, color='magenta', label='i = 0')\n",
    "axs[1].plot(x, y2, color='purple', label='i = 2')\n",
    "axs[2].plot(x, y3, color='cyan', label='i = 4')\n",
    "\n",
    "# Highlight specific word positions (x = 0, x = 7.5)\n",
    "highlight_x = [0, 8]\n",
    "highlight_y1 = [np.sin(10 * pos) for pos in highlight_x]\n",
    "highlight_y2 = [np.sin(5 * pos) for pos in highlight_x]\n",
    "highlight_y3 = [np.sin(2 * pos) for pos in highlight_x]\n",
    "\n",
    "# Highlight points on the curves for each axis\n",
    "axs[0].scatter(highlight_x, highlight_y1, color='magenta', s=100, zorder=5)\n",
    "axs[1].scatter(highlight_x, highlight_y2, color='purple', s=100, zorder=5)\n",
    "axs[2].scatter(highlight_x, highlight_y3, color='cyan', s=100, zorder=5)\n",
    "\n",
    "# Add dashed circles around points for each plot\n",
    "for x_val, y_val in zip(highlight_x, highlight_y1):\n",
    "    axs[0].add_patch(plt.Circle((x_val, y_val), 0.25, color='red', fill=False, linestyle='--', linewidth=2))\n",
    "\n",
    "for x_val, y_val in zip(highlight_x, highlight_y2):\n",
    "    axs[1].add_patch(plt.Circle((x_val, y_val), 0.25, color='red', fill=False, linestyle='--', linewidth=2))\n",
    "\n",
    "for x_val, y_val in zip(highlight_x, highlight_y3):\n",
    "    axs[2].add_patch(plt.Circle((x_val, y_val), 0.25, color='red', fill=False, linestyle='--', linewidth=2))\n",
    "\n",
    "# Add vertical lines at the highlight positions across all subplots\n",
    "for ax in axs:\n",
    "    for h_x in highlight_x[1:]:\n",
    "        ax.axvline(h_x, color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# Set labels for each subplot\n",
    "axs[0].set_ylabel('i = 0')\n",
    "axs[1].set_ylabel('i = 2')\n",
    "axs[2].set_ylabel('i = 4')\n",
    "axs[2].set_xlabel('Word Position')\n",
    "\n",
    "# Set grid and titles\n",
    "for ax in axs:\n",
    "    ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    ax.axhline(0, color='black',linewidth=1)\n",
    "    ax.axvline(0, color='black',linewidth=1)\n",
    "\n",
    "# Add a shared title for all subplots\n",
    "fig.suptitle('Position Embeddings', fontsize=16)\n",
    "\n",
    "# Show all word positions\n",
    "for ax in axs:\n",
    "    ax.set_xticks(np.arange(0, 11, 1))\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "id": "a6c0801728b74ca2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Dans la cellule suivante, implémenter un encodage de position similaire à ce qui est utilisé dans \"Attention Is All You Need\".\n",
    "Il faudra générer un tenseur de dimensions $(1, n, d)$, où $n$ est le nombre d'éléments et $d$ le nombre de dimension de $q_i$.\n",
    "Générez l'encodage selon cet encodage:\n",
    "\n",
    "$$\\text{PE}_{pos, 2i} = \\sin(\\text{pos} / 10000^{i/d})$$\n",
    "\n",
    "$$\\text{PE}_{pos, 2i + 1} = \\cos(\\text{pos} / 10000^{i/d})$$\n",
    "\n",
    "Où $\\text{pos}$ est la position dans la séquence (de 0 à $n - 1$) et $i$ est l'index dans le vecteur (de 0 à $d - 1$)."
   ],
   "id": "1c13039a72ae9164"
  },
  {
   "cell_type": "code",
   "id": "b32ebae2",
   "metadata": {},
   "source": [
    "def position_encoding(n_keys: int, dimension: int, device: torch.device = torch.device(\"cpu\")):\n",
    "    pos = torch.arange(n_keys, dtype=torch.float, device=device).reshape(1, -1, 1)\n",
    "    dim = torch.arange(dimension, dtype=torch.float, device=device).reshape(1, 1, -1)\n",
    "    # TODO\n",
    "    phase = ...\n",
    "\n",
    "    return torch.where(dim.long() % 2 == 0, torch.sin(phase), torch.cos(phase))\n",
    "\n",
    "pe = position_encoding(64, 128)\n",
    "plt.imshow(pe.squeeze())\n",
    "plt.xlabel('dimension')\n",
    "plt.ylabel('n')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1fe9690e",
   "metadata": {},
   "source": [
    "Dans la cellule suivante, ajoutez de l'encodage de position sur $q$ et $k$."
   ]
  },
  {
   "cell_type": "code",
   "id": "e4def6ec",
   "metadata": {},
   "source": [
    "q = F.normalize(torch.tensor([[3.0, 3.0], [-1, 3.2], [0.2, -0.4], [0, -1]]), dim=1).unsqueeze(0)\n",
    "k = F.normalize(torch.tensor([[2.0, 1.5], [-4, 3], [0, 3], [-1, -4]]), dim=1).unsqueeze(0)\n",
    "v = F.normalize(torch.tensor([[-1.0, 2], [3, 4], [2, -3], [0, 2]]), dim=1).unsqueeze(0)\n",
    "\n",
    "_, n, d = q.shape\n",
    "\n",
    "# TODO\n",
    "q = ...\n",
    "k = ...\n",
    "\n",
    "scaled_dot_product_attention(q, k, v)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a99bcec4",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "- Maintenant, que se passe-t'il lorsque l'on permute les $q_i$? et les $(k_i, v_i)$ après l'ajout de l'encodage de position?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8290c315",
   "metadata": {},
   "source": [
    "### Le \"Scaled\" dans Scaled Dot-Product Attention\n",
    "\n",
    "Le facteur de $\\frac{1}{\\sqrt{d}}$ peut sembler inutile, pourtant, il est crucial à l'optimisation des Transformers.\n",
    "Si on fait l'hypothèse que $q_i$ et $k_i$ suivent une distribution normale $\\mathcal{N}(0, 1)$, le produit entre $q$ et $k$ aura une variance de $d$.\n",
    "Il faut donc diviser par $\\sqrt{d}$ afin d'obtenir une variance de $1$.\n",
    "\n",
    "Dans la cellule suivante, calculez `attention` en faisant le produit entre $q$ et $k$ sans le facteur d'échelle.\n",
    "Ensuite, calculez `scaled_attention` en y ajoutant le facteur de $\\frac{1}{\\sqrt{d}}$.\n",
    "\n",
    "Comparez la variance dans les deux cas."
   ]
  },
  {
   "cell_type": "code",
   "id": "dd41f0c6",
   "metadata": {},
   "source": [
    "torch.manual_seed(1337)\n",
    "num_tokens = 32\n",
    "dimension = 256\n",
    "\n",
    "q = torch.randn(num_tokens, dimension)\n",
    "k = torch.randn(num_tokens, dimension)\n",
    "\n",
    "# TODO\n",
    "attention = ...\n",
    "# TODO\n",
    "scaled_attention = ...\n",
    "\n",
    "print(f'Variance sans scaling: {attention.var()}')\n",
    "print(f'Variance avec scaling: {scaled_attention.var()}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "61673f8d",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "- Justifiez pourquoi la variance est de $d$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09ef453",
   "metadata": {},
   "source": [
    "#### Impact sur les Transformers\n",
    "\n",
    "Quel est l'impact d'une grande variance sur la softmax?\n",
    "\n",
    "Comparez la sortie de la softmax des vecteurs `attention` et `scaled_attention` de la cellule précédente."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO\n",
    "low_variance_softmax = ...\n",
    "high_variance_softmax =  ...\n",
    "\n",
    "print(f'Softmax avec faible variance:\\n\\t{[round(x.item(), 5) for x in low_variance_softmax]}')\n",
    "print(f'Softmax avec haute variance:\\n\\t{[round(x.item(), 5) for x in high_variance_softmax]}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d882dccba104baa",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "53bd881e",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "- Que remarquez-vous du vecteur softmax lorsque la variance est grande?\n",
    "\n",
    "- Pourquoi est-ce que ceci serait mauvais le Transformer?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4658024",
   "metadata": {},
   "source": [
    "## Partie 2: Multi-Head Attention\n",
    "On va maintenant créer un module PyTorch utilisant l'attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea69c518",
   "metadata": {},
   "source": [
    "### AttentionHead\n",
    "On commence par définir `AttentionHead`, un module représentant une tête d'attention à l'intérieur d'un Transformer.\n",
    "- On projette `q`, `k`, et `v` avec des couches linéaires sans biais.\n",
    "- On applique du dropout sur la matrice d'attention après la softmax.\n",
    "\n",
    "Remplissez les TODOs dans le code suivant."
   ]
  },
  {
   "cell_type": "code",
   "id": "858e2f9a",
   "metadata": {},
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, dim_embed: int, head_size: int, dropout: float):\n",
    "        super().__init__()\n",
    "        # TODO trois couches linéaires sans biais\n",
    "        self.q_proj = ...\n",
    "        self.k_proj = ...\n",
    "        self.v_proj = ...\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        B, T, C = q.shape\n",
    "        # TODO appliquer les couches lineaires  à q, k et v\n",
    "        q = ...\n",
    "        k = ...\n",
    "        v = ...\n",
    "        \n",
    "        # TODO calculer la matrice d'attention\n",
    "        attention = ...\n",
    "        \n",
    "        # TODO\n",
    "        if mask is not None:\n",
    "            # Appliquer le masque\n",
    "            ...\n",
    "    \n",
    "        softmax = ...\n",
    "        \n",
    "        # Appliquer du dropout après la softmax\n",
    "        softmax = self.dropout(softmax)\n",
    "        \n",
    "        # TODO\n",
    "        return\n",
    "\n",
    "    \n",
    "head = AttentionHead(16, 32, 0.1)\n",
    "x = torch.randn((1, 6, 16))\n",
    "output = head(x, x, x)\n",
    "\n",
    "print(f'Shape \\t\\t= {tuple(output.shape)}')\n",
    "print(f'Expected shape \\t= {(1, 6, 32)}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4bb99765",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "On définit maintenant le module `MultiHeadAttention` qui utilise plusieurs têtes d'attention séparées.\n",
    "- Plusieurs têtes d'attention.\n",
    "- Concaténation des sorties de chaque tête\n",
    "- Couche linéaire\n",
    "- Dropout\n",
    "\n",
    "Remplissez les TODOs dans le code suivant."
   ]
  },
  {
   "cell_type": "code",
   "id": "005990f2",
   "metadata": {},
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, dim_embed: int, head_size: int, dropout: float):\n",
    "        super().__init__()\n",
    "        # TODO créer une liste de `num_heads` modules AttentionHead\n",
    "        self.heads = nn.ModuleList(...)\n",
    "        self.fc = nn.Linear(num_heads * head_size, dim_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        out = torch.cat([h(q, k, v, mask) for h in self.heads], dim=-1)\n",
    "        # TODO appliquer fc suivit de dropout\n",
    "        return\n",
    "\n",
    "    \n",
    "m_head = MultiHeadAttention(3, 32, 256, 0.1)\n",
    "x = torch.randn((1, 6, 32))\n",
    "output = m_head(x, x, x)\n",
    "\n",
    "print(f'Shape \\t\\t= {tuple(output.shape)}')\n",
    "print(f'Expected shape \\t= {(1, 6, 32)}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8653e212",
   "metadata": {},
   "source": [
    "## Partie 3: TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a82fb3d",
   "metadata": {},
   "source": [
    "### FeedForward\n",
    "On définit maintenant le réseau `FeedForward` utilisé dans le Transformer.\n",
    "\n",
    "- Linear: dim_model -> 4 * dim_model\n",
    "- ReLU\n",
    "- Linear: 4 * dim_model -> dim_model\n",
    "- Dropout"
   ]
  },
  {
   "cell_type": "code",
   "id": "05e97326",
   "metadata": {},
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim_model: int, dropout: float):\n",
    "        super().__init__()\n",
    "        # TODO Créer le réseau selon les indications de la cellule précédente\n",
    "        self.ffn = nn.Sequential(\n",
    "            ...\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.ffn(x)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e9ad204d",
   "metadata": {},
   "source": [
    "### TransformerEncoderLayer\n",
    "On peut maintenant construire une couche du Transformer.\n",
    "\n",
    "- LayerNorm\n",
    "- MultiHeadAttention\n",
    "- Connexion résiduelle\n",
    "- LayerNorm\n",
    "- FeedForward\n",
    "- Connexion résiduelle"
   ]
  },
  {
   "cell_type": "code",
   "id": "6ff2d9dd",
   "metadata": {},
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, dim_model: int, num_head: int, dropout: float):\n",
    "        super().__init__()\n",
    "        head_size = dim_model // num_head\n",
    "        self.attention = MultiHeadAttention(num_head, dim_model, head_size, dropout)\n",
    "        self.ffn = FeedForward(dim_model, dropout)\n",
    "        self.norm1_q = nn.LayerNorm(dim_model)\n",
    "        self.norm1_k = nn.LayerNorm(dim_model)\n",
    "        self.norm1_v = nn.LayerNorm(dim_model)\n",
    "        self.norm2 = nn.LayerNorm(dim_model)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # TODO appliquer LayerNorm à q, k et v\n",
    "        q = ...\n",
    "        k = ...\n",
    "        v = ...\n",
    "        \n",
    "        q = q + self.attention(q, k, v, mask)\n",
    "        q = q + self.ffn(self.norm2(q))\n",
    "        return q\n",
    "    \n",
    "transf = TransformerEncoderLayer(64, 2, 0.1)\n",
    "x = torch.randn((1, 6, 64))\n",
    "output = transf(x, x, x)\n",
    "\n",
    "print(f'Shape \\t\\t= {tuple(output.shape)}')\n",
    "print(f'Expected shape \\t= {(1, 6, 64)}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "93c5bebb",
   "metadata": {},
   "source": [
    "## Partie 4: TransformerEncoder\n",
    "On peut maintenant tout mettre ensemble et construire le `TransformerEncoder`!"
   ]
  },
  {
   "cell_type": "code",
   "id": "bfb6b0ff",
   "metadata": {},
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers: int, dim_model: int, num_heads: int, out_size: int, dropout: float):\n",
    "        super().__init__()\n",
    "        # TODO Créer une liste de `num_layers` modules TransformerEncoderLayer\n",
    "        self.layers = nn.ModuleList(...)\n",
    "        self.norm = nn.LayerNorm(dim_model)\n",
    "        self.fc = nn.Linear(dim_model, out_size)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, num_tokens, dim = x.shape\n",
    "        # TODO ajouter l'encodage de position avec `position_encoding`\n",
    "        x += ...\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            # TODO appeler layer avec x comme valeur de q, k et v, ne pas oublier de passer le mask\n",
    "            x = ...\n",
    "            \n",
    "        x = self.norm(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "x = torch.randn((2, 64, 64))\n",
    "\n",
    "transformer_enc = TransformerEncoder(6, 64, 2, 10, 0.0)\n",
    "output = transformer_enc(x)\n",
    "\n",
    "print(f'Shape \\t\\t= {tuple(output.shape)}')\n",
    "print(f'Expected shape \\t= {(2, 64, 10)}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "061452ec",
   "metadata": {},
   "source": [
    "## Partie 5: Entraînement\n",
    "On entraîne maintenant notre Transformer à prédire du texte.\n",
    "- Inspiré par https://github.com/karpathy/ng-video-lecture/tree/master de Andrej karpathy\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e14995a9",
   "metadata": {},
   "source": [
    "# Télécharger les données localement\n",
    "!curl -o ~/GLO-4030/datasets/tiny_shakespeare.txt https://raw.githubusercontent.com/karpathy/ng-video-lecture/master/input.txt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Sur Google Colab\n",
    "import requests\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/karpathy/ng-video-lecture/master/input.txt'\n",
    "res = requests.get(url)\n",
    "text = res.content.decode('utf-8')\n",
    "\n",
    "# Extrait des données\n",
    "print(text[:147])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f84289ca55c4c4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "129b76b8",
   "metadata": {},
   "source": [
    "# Sur Calcul Québec et localement\n",
    "# Charger les données\n",
    "data_path = pathlib.Path('~/GLO-4030/datasets/tiny_shakespeare.txt').expanduser()\n",
    "with open(data_path, 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Extrait des données\n",
    "print(text[:147])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "77e78d6f",
   "metadata": {},
   "source": [
    "# Calculer le vocabulaire (les caractères utilisés dans le texte)\n",
    "characters = sorted(list(set(text)))\n",
    "vocab_size = len(characters)\n",
    "\n",
    "str_to_int = {c:i for i, c in enumerate(characters)}\n",
    "int_to_str = {v: k for k, v in str_to_int.items()}\n",
    "\n",
    "# Fonction utilitaires pour convertir les caractères en entier, et inversement\n",
    "def encode(s):\n",
    "    return [str_to_int[c] for c in s]\n",
    "\n",
    "def decode(s):\n",
    "    return [int_to_str[c] for c in s]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9459bc61",
   "metadata": {},
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "batch_size = 128\n",
    "\n",
    "# Générer une batch aléatoire\n",
    "def get_batch(split, block_size, device):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    indices = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in indices])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in indices])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "print(get_batch('train', 256, 'cpu'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1b1d3aac",
   "metadata": {},
   "source": [
    "class TransformerNLP(nn.Module):\n",
    "    def __init__(self, num_layers: int, dim_model: int, num_heads: int, out_size: int, dropout: float, vocab_size: int, block_size: int):\n",
    "        super().__init__()\n",
    "        # Embedding pour convertir des entiers en vecteurs\n",
    "        # Il s'agit essentiellement d'un tableau associant chaque entier à un vecteur\n",
    "        self.embedding = nn.Embedding(vocab_size, dim_model)\n",
    "        # Notre transformer!\n",
    "        self.transformer = TransformerEncoder(num_layers, dim_model, num_heads, out_size, dropout)\n",
    "        # Masque pour éviter que les lettre puissent porter attention aux lettres subséquentes\n",
    "        # On ne veut permettre aux lettres que de porter attention aux lettres précédentes\n",
    "        # On utilise `tril` afin de générer une matrice triangulaire inférieure\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        # TODO appliquer l'embedding\n",
    "        x = ...\n",
    "        \n",
    "        # TODO encodage de position\n",
    "        x = ...\n",
    "        \n",
    "        # TODO passer x dans le transformer\n",
    "        x = ...\n",
    "        return x\n",
    "        \n",
    "    def loss(self, pred, y):\n",
    "        B, T, C = pred.shape\n",
    "        pred = pred.view(B * T, C)\n",
    "        y = y.view(B * T)\n",
    "        return F.cross_entropy(pred, y)\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, block_size):\n",
    "        # Génère du texte avec le modèle\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fd5670ec",
   "metadata": {},
   "source": [
    "block_size = 256\n",
    "max_iters = 10_000\n",
    "eval_interval = 1000\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.1\n",
    "\n",
    "# Model\n",
    "model = TransformerNLP(n_layer, n_embd, n_head, vocab_size, dropout, vocab_size, block_size)\n",
    "# Load a pretrained model\n",
    "model.load_state_dict(torch.load('checkpoints/transformer_nlp.pth', weights_only=True))\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3705d011",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "iter_loss = []\n",
    "for i in range(max_iters):\n",
    "    if i % eval_interval == 0 or i == max_iters - 1:\n",
    "        with torch.no_grad():\n",
    "            out = {}\n",
    "            model.eval()\n",
    "            for split in ['train', 'val']:\n",
    "                losses = torch.zeros(eval_iters)\n",
    "                for k in range(eval_iters):\n",
    "                    x, y = get_batch(split, block_size, device)\n",
    "                    pred = model(x)\n",
    "                    loss = model.loss(pred, y)\n",
    "                    losses[k] = loss.item()\n",
    "                out[split] = losses.mean()\n",
    "            print(f\"step {i}: train loss {out['train'].item():.4f}, val loss {out['val'].item():.4f}\")\n",
    "            iter_loss.append((out['train'].item(), out['val'].item()))\n",
    "            model.train()\n",
    "    x, y = get_batch('train', block_size, device)\n",
    "        \n",
    "    pred = model(x)\n",
    "    loss = model.loss(pred, y)\n",
    "        \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "45f163fe",
   "metadata": {},
   "source": [
    "iter_loss_np = np.array(iter_loss)\n",
    "plt.plot(iter_loss_np[:, 0], label='train')\n",
    "plt.plot(iter_loss_np[:, 1], label='valid')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "60bc695f",
   "metadata": {},
   "source": [
    "# Prediction du texte\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(''.join(decode(model.generate(context, 500, block_size)[0].tolist())))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aca9a706",
   "metadata": {},
   "source": [
    "# Sauvegarde des poids\n",
    "torch.save(model.state_dict(), pathlib.Path('~/GLO-4030/datasets/transformer_nlp.pth').expanduser())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e5863590",
   "metadata": {},
   "source": [
    "## Partie 6: Vision Transformer\n",
    "\n",
    "Les Transformers ne sont pas limités au traitement de la langue naturelle.\n",
    "Plutôt que de passer des lettres ou des mots, on peut passer des portions d'images encodés en vecteur.\n",
    "\n",
    "Pour cette partie, nous allons entraîner un ViT (Vision Transformer) sur CIFAR10.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "fd8fa741",
   "metadata": {},
   "source": [
    "import torchvision.models\n",
    "\n",
    "import poutyne as pt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Supprimer le dernier modele\n",
    "def find_cuda_tensors():\n",
    "    \"\"\"Finds all Python variables that are tensors on CUDA.\"\"\"\n",
    "    cuda_tensors = {}\n",
    "    for var_name, var_value in globals().items():\n",
    "        if isinstance(var_value, torch.Tensor) and var_value.is_cuda:\n",
    "            cuda_tensors[var_name] = var_value\n",
    "    return cuda_tensors\n",
    "\n",
    "for tensor_name, tensor in find_cuda_tensors().items():\n",
    "    del globals()[tensor_name]\n",
    "\n",
    "if 'model' in globals():\n",
    "    del model\n",
    "if 'optimizer' in globals():\n",
    "    del optimizer\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n"
   ],
   "id": "45c6409098300c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b056f4a0",
   "metadata": {},
   "source": [
    "# Data\n",
    "batch_size = 128\n",
    "\n",
    "transform_train = T.Compose([\n",
    "    T.RandAugment(4, 14),\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "cifar, cifar_test = ddatasets.load_cifar10()\n",
    "cifar.transform = transform_train\n",
    "cifar_test.transform = transform_test\n",
    "\n",
    "train_split = 0.8\n",
    "num_data = len(cifar)\n",
    "indices = np.arange(num_data)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "split = math.floor(train_split * num_data)\n",
    "train_idx, valid_idx = indices[:split], indices[split:]\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(cifar, train_idx)\n",
    "valid_dataset = torch.utils.data.Subset(cifar, valid_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(cifar_test, batch_size=batch_size)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dfe437b0",
   "metadata": {},
   "source": [
    "pathlib.Path('logs').mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "epoch = 100\n",
    "learning_rate = 1e-4\n",
    "\n",
    "img_size = cifar[0][0].shape[-1]\n",
    "patch_size = 4\n",
    "num_layers = 12\n",
    "num_heads = 12\n",
    "hidden_dim = 768\n",
    "mlp_dim = 3072\n",
    "\n",
    "model = torchvision.models.VisionTransformer(img_size, patch_size, num_layers, num_heads, hidden_dim, mlp_dim, dropout=0.1, attention_dropout=0.1, num_classes=10)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model = pt.Model(model, optimizer, 'cross_entropy', batch_metrics=['accuracy'])\n",
    "model.cuda()\n",
    "\n",
    "history = model.fit_generator(train_loader, valid_loader, epochs=epoch, callbacks=[\n",
    "    pt.ModelCheckpoint('logs/best_epoch_{epoch}.ckpt', monitor='val_acc', mode='max', save_best_only=True, restore_best=True, verbose=True, temporary_filename='best_epoch.ckpt.tmp'),\n",
    "])\n",
    "\n",
    "dtrain.History(history).display()\n",
    "\n",
    "test_loss, test_acc = model.evaluate_generator(test_loader)\n",
    "print('test_loss: {:.4f} test_acc: {:.2f}'.format(test_loss, test_acc))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d9573bdc",
   "metadata": {},
   "source": [
    "model.load_weights('logs/best_epoch_5.ckpt', weights_only=True)\n",
    "test_loss, test_acc = model.evaluate_generator(test_loader)\n",
    "print('Epoch 5: test_loss: {:.4f} test_acc: {:.2f}'.format(test_loss, test_acc))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3e9e53a0",
   "metadata": {},
   "source": [
    "## Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5de7174",
   "metadata": {},
   "source": [
    "### Transformers annotés\n",
    "\n",
    "On vous recommande de consulter [willGuimont/transformers](https://github.com/willGuimont/transformers) pour mieux comprendre les modèles de Transformer.\n",
    "Le projet implémente plusieurs architectures basées sur les Transformers; le code est bien commenté et devrait être facile à suivre.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42068c2",
   "metadata": {},
   "source": [
    "### Self-attention vs Cross-attention\n",
    "\n",
    "Dans les exemples utilisés dans ce laboratoire, $q$, $k$ et $v$ sont dérivés d'une seule valeur (`x = layer(x, x, x, mask)` dans le `TransformerEncoder`.\n",
    "Ceci est appelé du \"self-attention\".\n",
    "\n",
    "Il est aussi possible de faire du \"cross-attention\", lorsque $q$ et $(k, v)$ proviennent de données différentes.\n",
    "\n",
    "[Perceiver](https://www.deepmind.com/publications/perceiver-general-perception-with-iterative-attention) et [PerceiverIO](https://www.deepmind.com/open-source/perceiver-io) sont de beaux exemples d'utilisation du \"cross-attention\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825829c8",
   "metadata": {},
   "source": [
    "### TransformerDecoder\n",
    "\n",
    "Voici une implémentation du TransformerDecoder de \"Attention Is All You Need\".\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6be3c735",
   "metadata": {},
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, dim_model: int, num_head: int, dropout: float):\n",
    "        super().__init__()\n",
    "        head_size = dim_model // num_head\n",
    "        self.self_attention = MultiHeadAttention(num_head, dim_model, head_size, dropout)\n",
    "        self.norm1_target = nn.LayerNorm(dim_model)\n",
    "        self.norm1_memory = nn.LayerNorm(dim_model)\n",
    "        \n",
    "        self.norm2 = nn.LayerNorm(dim_model)\n",
    "        self.cross_attention = MultiHeadAttention(num_head, dim_model, head_size, dropout)\n",
    "        \n",
    "        self.norm3 = nn.LayerNorm(dim_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ffn = FeedForward(dim_model, dropout)\n",
    "    \n",
    "    def forward(self, target, memory, self_attention_mask=None, cross_attention_mask=None):\n",
    "        target, memory = self.norm1_target(target), self.norm1_memory(memory)\n",
    "        target = target + self.self_attention(target, target, target, self_attention_mask)\n",
    "        target = target + self.cross_attention(self.norm2(target), memory, memory, cross_attention_mask)\n",
    "        return target + self.ffn(self.norm3(target))\n",
    "    \n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, num_layers: int, dim_model: int, num_heads: int, out_size: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([TransformerDecoderLayer(dim_model, num_heads, dropout) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(dim_model)\n",
    "        self.fc = nn.Linear(dim_model, out_size)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "    def forward(self, target, memory, self_attention_mask=None, cross_attention_mask=None):\n",
    "        num_tokens, dim = target.shape[1], target.shape[2]\n",
    "        target += position_encoding(num_tokens, dim, target.device)\n",
    "        for layer in self.layers:\n",
    "            target = layer(target, memory, self_attention_mask, cross_attention_mask)\n",
    "        target = self.norm(target)\n",
    "        target = self.fc(target)\n",
    "        return target\n",
    "    \n",
    "dec = TransformerDecoder(6, 10, 5, 100, 0.0)\n",
    "target = torch.randn((2, 5, 10))\n",
    "memory = torch.randn((2, 5, 10))\n",
    "print(dec(target, memory).shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "225e87e2",
   "metadata": {},
   "source": [
    "### Exemple d'implémentation de ViT"
   ]
  },
  {
   "cell_type": "code",
   "id": "d0de7a71",
   "metadata": {},
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size: int, patch_size: int, num_classes: int, channels: int, num_layers: int, dim_model: int, num_heads: int, out_size: int, dropout: float):\n",
    "        super().__init__()\n",
    "        assert img_size % patch_size == 0, \"img_size must be divisible by patch_size\"\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_dim = channels * patch_size * patch_size\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(self.patch_dim)\n",
    "        self.fc1 = nn.Linear(self.patch_dim, dim_model)\n",
    "        self.norm2 = nn.LayerNorm(dim_model)\n",
    "        \n",
    "        self.cls = nn.Parameter(torch.randn((1, 1, dim_model)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.trans_enc = TransformerEncoder(num_layers, dim_model, num_heads, out_size, dropout)\n",
    "        \n",
    "        self.norm3 = nn.LayerNorm(dim_model)\n",
    "        self.fc2 = nn.Linear(dim_model, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        size = 8\n",
    "        stride = 1\n",
    "        \n",
    "        # Image to patches\n",
    "        x = self._img_to_patch_emdedding(x)\n",
    "        \n",
    "        # Add cls token\n",
    "        cls = ein.repeat(self.cls, '1 1 d -> b 1 d', b=batch_size)\n",
    "        x = torch.cat((cls, x), dim=1)\n",
    "        \n",
    "        # Positional encoding\n",
    "        batch_size, num_tokens, dim = x.shape\n",
    "        x += position_encoding(num_tokens, dim, x.device)\n",
    "        \n",
    "        # Dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Transformer\n",
    "        x = self.trans_enc(x)\n",
    "\n",
    "        \n",
    "        # Final prediction from cls token\n",
    "        x = x[:, 0]\n",
    "        x = self.fc2(self.norm3(x))\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def _img_to_patch_emdedding(self, img):\n",
    "        x = ein.rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=self.patch_size, p2=self.patch_size)\n",
    "        x = self.norm1(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.norm2(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "131baa3a",
   "metadata": {},
   "source": [
    "epoch = 100\n",
    "batch_size = 256\n",
    "learning_rate = 1e-4\n",
    "\n",
    "img_size = 32\n",
    "patch_size = 8\n",
    "num_classes = 10\n",
    "channels = 3\n",
    "num_layers = 6\n",
    "dim_model = 512\n",
    "num_heads = 12\n",
    "out_size = dim_model\n",
    "dropout = 0.1\n",
    "\n",
    "model = VisionTransformer(img_size, patch_size, num_classes, channels, num_layers, dim_model, num_heads, out_size, dropout)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model = pt.Model(model, optimizer, 'cross_entropy', batch_metrics=['accuracy'])\n",
    "model.cuda()\n",
    "\n",
    "history = model.fit_generator(train_loader, valid_loader, epochs=epoch, callbacks=[\n",
    "    pt.ModelCheckpoint('logs/vit_best_epoch_{epoch}.ckpt', monitor='val_acc', mode='max', save_best_only=True, restore_best=True, verbose=True, temporary_filename='best_epoch.ckpt.tmp'),\n",
    "])\n",
    "\n",
    "dtrain.History(history).display()\n",
    "\n",
    "test_loss, test_acc = model.evaluate_generator(test_loader)\n",
    "print('test_loss: {:.4f} test_acc: {:.2f}'.format(test_loss, test_acc))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "86b1d6d0",
   "metadata": {},
   "source": [
    "### Attention comme passage de messages\n",
    "- L'attention peut être interprété comme du passage de message sur un graphe.\n",
    "- Chaque noeud $i$ dans le graphe possède un triplet $(q_i, k_i, v_i$)\n",
    "- Grâce à $k_i$, chaque noeud \"diffuse\" ce qu'il connait\n",
    "- Grâce à $q_i$, chaque noeud demande aux autres noeuds de l'information\n",
    "- Selon ce que l'affinité entre les $q_i$ et les $k_i$, chaque noeud aggrège l'information $v_i$ de ses voisins\n",
    "- Champ réceptif large, chaque noeud à accès aux autres noeuds en une seule opération.\n",
    "\n",
    "<div>\n",
    "<img src=\"docs/simplex_graph.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "- Cette interprétation est cohérente avec la notion que les transformers sont des fonctions d'ensemble vers ensemble (invariance à l'ordre des éléments), la position des noeuds n'a pas d'importance, on doit encoder l'information sur la position autrement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
